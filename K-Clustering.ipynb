{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filedata(filenames):\n",
    "    #Read file data and return data frame\n",
    "    dfs = []\n",
    "    for filename in filenames:\n",
    "        original_df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        dfs.append(original_df)\n",
    "    return dfs\n",
    "\n",
    "def time_series_data(dataframes, target_col=None):\n",
    "    #Read the data frames and make them array in chronological order\n",
    "    tsdata = []\n",
    "    for i, df in enumerate(dataframes):\n",
    "        tsdata.append(df[target_col].values.tolist()[:])\n",
    "        #Check the maximum length of each time series data\n",
    "        len_max = 0\n",
    "        for ts in tsdata:\n",
    "            if len(ts) > len_max:\n",
    "                len_max = len(ts)\n",
    "        # Add last data to align the lengths of time series data\n",
    "        for i, ts in enumerate(tsdata):\n",
    "            len_add = len_max - len(ts)\n",
    "            tsdata[i] = ts + [ts[-1]] * len_add\n",
    "    \n",
    "    tsdata = np.array(tsdata)\n",
    "    return tsdata\n",
    "\n",
    "def transform_vector(time_series_array):\n",
    "    #Convert to vector\n",
    "    stack_list = []\n",
    "    for j in range(len(time_series_array)):\n",
    "        data = np.array(time_series_array[j])\n",
    "        data = data.reshape((1, len(data))).T\n",
    "        stack_list.append(data)\n",
    "    # Make one dimensional array\n",
    "    stack_data = np.stack(stack_list, axis=0)\n",
    "    return stack_data\n",
    "\n",
    "\n",
    "#filenames = sorted(glob.glob('sample/sample_data*.csv'))\n",
    "#df = read_filedata(filenames=filenames)\n",
    "#tsdata = time_series_data(dataframes=df, target_col='data')\n",
    "#stack_data = transform_vector(time_series_array=tsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "import csv\n",
    "\n",
    "results = []\n",
    "\n",
    "with open('test_huge.csv','r') as f:\n",
    "    lines = csv.reader(f)\n",
    "    for line in lines:\n",
    "        results.append([[float(i)] for i in line])\n",
    "\n",
    "stack_data = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft-DTW-k-means\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "stack_data = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(stack_data)\n",
    "\n",
    "sdtw_km = TimeSeriesKMeans(n_clusters=4, metric=\"softdtw\", metric_params={\"gamma_sdtw\": .01},\n",
    "                           verbose=True, random_state=seed)\n",
    "y_pred = sdtw_km.fit_predict(stack_data)\n",
    "\n",
    "#Clustering and visualization\n",
    "plt.figure(figsize=(30,36))\n",
    "for yi in range(4):\n",
    "    plt.subplot(4, 1, 1 + yi)\n",
    "    for xx in stack_data[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.title(\"Cluster %d\" % (yi + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# k-shape\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "#To calculate cross-correlation, you need to normalize.\n",
    "#TimeSeriesScalerMeanVariance is the class that normalizes the data.\n",
    "stack_data = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(stack_data)\n",
    "\n",
    "#Instantiation of the KShape class \n",
    "ks = KShape(n_clusters=5, n_init=10, verbose=True, random_state=seed)\n",
    "y_pred = ks.fit_predict(stack_data)\n",
    "\n",
    "#Clustering and visualization \n",
    "plt.figure(figsize=(30,36))\n",
    "for yi in range(5):\n",
    "    plt.subplot(5, 1, 1 + yi)\n",
    "    for xx in stack_data[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    #plt.plot(ks.cluster_centers_[yi].ravel(), \"r-\")\n",
    "    plt.title(\"Cluster %d\" % (yi + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export Clusters Results\n",
    "\n",
    "y_pred\n",
    "df = pd.DataFrame(y_pred)\n",
    "df.to_csv(\"file_path.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the number of clusters by the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "\n",
    "#Calculate 1 to 10 clusters \n",
    "for i  in range(1,11):\n",
    "    ks = KShape(n_clusters=i, n_init=10, verbose=True, random_state=seed)\n",
    "    #Execute clustering calculation\n",
    "    ks.fit(stack_data)\n",
    "    #ks.fit You can get SSE with #inertia_\n",
    "    distortions.append(ks.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
